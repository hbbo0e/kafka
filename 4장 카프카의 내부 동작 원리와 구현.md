
# 4.1 카프카 리플리케이션

카프카는 무수히 많은 데이터 파이프라인의 정중앙에 위치하는 메인 허브 역할을 한다. 만약, 하드웨어의 문제나 점검 등으로 인해 정상적으로 동작하지 못하거나, 연결된 전체 데이터 파이프라인에 영향을 미친다면 심각한 문제로 이어진다.
따라서 브로커 한 두 대에서 장애가 발생하더라도 중앙 데이터 허브로서 안정적인 서비스가 운영될 수 있도록, 리플리케이션이라는 동작을 하게 되었다.

## 4.1.1 리플리케이션 동작 개요

카프카의 리플리케이션 동작을 위해 토픽 생성 시 필숫값으로 `replication factor` 옵션을 설정해야 한다.

> 명령어 실행 후 토픽이 생성됐다는 메시지를 확인한 다음 `describe` 옵션을 이용해 토픽의 상세보기를 출력한다.

![[Pasted image 20240614191048.png]]
	1. `peter-test01` 토픽의 파티션 수인 1과 리플리케이션 팩터 수인 3이 표시되어 있다.
	2. `peter-test01` 토픽의 파티션0에 대한 상세 내용으로, 파티션0의 리더는 브로커1이다. 리플리케이션들은 브로커 1, 2, 3에 있다.
		현재 동기화되고 있는 리플리케이션들은 브로커 1, 2, 3 이라는 의미이다.
		중요한 것은, 실제로 리플리케이션되는 것은 토픽이 아니라 토픽을 구성하는 각각의 파티션들이다.

> 콘솔 프로듀서를 이용해 `testmessage1` 이라는 메세지를 `peter-test01` 토픽으로 전송한다.
> -> `kafka-console-producer.sh` 사용

![[Pasted image 20240614191339.png]]

> 세그먼트 파일에 저장되어 있는지 확인해보자
> -> `kafka-dump-log.sh`

![[Pasted image 20240614191426.png]]
	1. 시작 오프셋 위치는 0이다.
	2. 메시지 카운트는 1이다
	3. 프로듀서를 통해 보낸 메세지는 test message1 임을 알 수 있다.

- 현재 접속한 서버는 `peter-kafka01` 이지만, 카프카 클러스터를 이루는 다른 브로커들인 `peter-kafka02, 03` 에 접속해서 명령어를 실행하더라도 모든 브로커가 동일한 메세지를 갖고 있음을 알 수 있다.

## 4.1.2 리더와 팔로워

파티션의 리더는 리플리케이션 중 하나가 선정되며, 모든 읽기와 쓰기는 그 리더를 통해서만 가능하다. 
-> 프로듀서는 모든 리플리케이션에 메세지를 보내는 것이 아니라 리더에게만 메세지를 전송한다. 또한 컨슈머도 오직 리더로부터 메세지를 가져오게 된다.

![[Pasted image 20240614201531.png]]
	프로듀서는 `peter-test01` 토픽으로 메세지를 전송하고, 파티션의 리더만 읽고 쓰기가 가능하기 때문에 0번 파티션의 리더로 메세지를 보내게 되고, 컨슈머 동작에서도 0번 파티션의 리더로부터 메세지를 가져온다.
	나머지 팔로워들은 이슈를 대비해 언제든지 새로운 리더가 될 준비를 해야 하기 때문에, 파티션의 리더가 새로운 메세지를 받았는지 확인하고 새로운 메세지가 있다면 해당 메세지를 리더로부터 복제한다.

## 4.1.3 복제 유지와 커밋

> 리더와 팔로워

이들은 ISR (InSyncReplica) 라고 하는 논리적 그룹으로 묶여 있다. 리더와 팔로워를 별도의 그룹으로 나눈 것은 해당 그룹 안에 속한 팔로워들만이 새로운 리더의 자격을 가질 수 있기 때문이다.

ISR 내의 팔로워들은 리더와 데이터 일치를 유지하기 위해 지속적으로 리더의 데이터를 따라가게 되고, 리더는 ISR 내 모든 팔로워가 메시지를 받을 때까지 기다린다. 기타 이슈 때문에 뒤처진 팔로워는 이미 리더와의 데이터가 불일치 하기 때문에, 파티션의 리더는 뒤처지지 않고 리플리케이션 동작을 잘하고 있는지를 감시한다.

> 리더는 읽고 쓰는 동작 뿐만 아니라 팔로워가 리플리케이션 동작을 잘 수행하고 있는지 역시 판단한다.

만약 팔로워가 뒤처지고 있다면 리더는 리플리케이션 동작에 문제가 발생했다고 판단해 ISR 그룹에서 추방한다.
토픽 상세보기 명령어를 통해 현재 ISR 상태를 점검해봄으로써 현재 토픽의 상태가 양호한지 확인해볼 수 있다.

ISR 내에 모든 팔로워가 복제 완료되면, 리더는 내부적으로 커밋되었다는 표시를 하게 된다.

> 마지막 커밋 오프셋 위치는 하이워터마크라고 한다.

이렇게 커밋된 메시지만 컨슈머가 읽어갈 수 있다. 

![[Pasted image 20240614203946.png]]
	프로듀서가 처음 전송한 `test message1` 은 팔로워 모두가 리플리케이션 동작을 통해 저장하고 커밋을 완료하였고 `test message2` 는 팔로워들이 아직 리플리케이션 동작을 하기 전 상태이다.

> 위 상황에서 커밋되기 전 메세지를 컨슈머가 읽을 수 있다고 가정하고 어떻게 진행되는지 보자

![[Pasted image 20240614212156.png]]
	1. `컨슈머 A` 는 peter-test01 토픽을 컨슘한다.
	2. `컨슈머 A` peter-test01 토픽의 파티션 리더로부터 메세지를 읽어간다 -> test message1, test message2
	3. peter-test01 토픽의 파티션 리더가 있는 브로커에 문제가 발생해 팔로워 중 하나가 새로운 리더가 된다.
	4. 프로듀서가 보낸 `test message2` 메세지는 아직 팔로워들에게 리플리케이션 되지 않은 상태에서 새로운 리더로 변경됐으므로, 새로운 리더는 `test message1` 만 가지고 있다.
	5. 새로운 `컨슈머 B` 가 peter-test01 토픽을 컨슘한다.
	6. 새로운 리더로부터 메세지를 읽고, 메세지는 `test message1` 이다.

- 이런 식으로 메세지가 일치하지 않는 현상이 발생할 수 있기 때문에 카프카에서는 커밋된 메세지만 컨슈머가 읽어갈 수 있도록 구현되어 있다.

> 커밋된 위치는 어떻게 알 수 있을까?

모든 브로커는 재시작될 때, 커밋된 메세지를 유지하기 위해 로컬 디스크의 `replication-offset-checkpoint` 라는 파일에 마지막 커밋 오프셋 위치를 저장한다. 
	`replication-offset-checkpoint` 파일은 브로커 설정 파일에서 설정한 로그 디렉토리 경로에 있으며, 브로커 설정 파일의 로그 디렉토리는 `/data/kafka-logs` 로 설정되어 있고, 해당 디렉토리 하위에 위치한다.

![[Pasted image 20240614212820.png]]

> 커밋된 오프셋 번호가 증가하는지 확인하기 위해 콘솔 프로듀서를 이용해 test message2 메세지를 전송한 뒤 cat 명령어로 다시 해당 파일을 확인한다.
> 	`peter-test01` = 토픽 이름 `0` = 파티션 번호 `1` = 커밋된 오프셋 번호

![[Pasted image 20240614212947.png]]


## 4.1.4 리더와 팔로워의 단계별 리플리케이션 동작

이런 식으로 바쁜 리더가 리플리케이션 동작을 위해 팔로워들과 많은 통신을 주고 받거나 리플리케이션 동작에 많은 관여를 한다면, 결과적으로 리더의 성능은 떨어지게 될 것이다.

![[Pasted image 20240617151254.png]]

현 상태에서 리더는 모든 팔로워가 0번 오프셋 메세지를 리플리케이션 하기 위한 요청을 보냈다는 사실을 알고 있다. 하지만, 리더는 팔로워들이 0번 오프셋에 대한 리플리케이션 동작을 성공했는지 실패했는지 여부를 알지 못한다.
	- **전통적인 메시징 큐 시스템인 래빗MQ**의 트랜잭션 모드에서는 모든 미러(카프카에서 팔로워) 가 메세지를 받았는지에 대한 ACK 를 리더에게 리턴하므로, 리더는 미러들이 메시지를 받았는지 알 수 있다.
	- **카프카**의 경우, 리더와 팔로워 사이에서 ACK 를 주고 받는 통신이 없다. 오히려 카프카는 리더와 팔로워 사이에 ACK 통신을 제거함으로써 리플리케이션 동작의 성능을 더욱 높였다.

![[Pasted image 20240617151538.png]]

리더는 1번 오프셋의 위치에 두 번째 새로운 메세지인 message2 를 프로듀서로부터 받은 뒤 저장한다.
팔로워들로부터 1번 오프셋에 대한 리플리케이션 요청을 받은 리더는 팔로워들의 0번 오프셋에 대한 리플리케이션 동작이 성공했음을 인지하고, 오프셋 0에 대해 커밋 표시를 한 후 하이워터마크를 증가시킨다.

리더는 팔로워들이 보내는 리플리케이션 요청의 오프셋을 보고 팔로워들이 어느 위치의 오프셋까지 리플리케이션을 성공했는지 인지할 수 있다.

![[Pasted image 20240617151903.png]]

리더의 응답을 받은 모든 팔로워는 0번 오프셋 메세지가 커밋되었다는 사실을 인지하게 되고, 리더와 동일하게 커밋을 표시한다.

- 여타 메시징 시스템들은 리플리케이션 동작에서 리더와 팔로워가 메시지를 잘 받았는지 확인하는 ACK 통신을 하지만, 카프카는 ACK 통신 단계를 제거했다는 사실이다.
- 카프카는 ACK 통신 단계를 제외했음에도 팔로워와 리더 간의 리플리케이션 동작이 매우 빠르면서도 신뢰할 수 있다는 것이다.
- 리플리케이션 동작 방식은 리더가 푸시하는 방식이 아니라 팔로워들이 풀 하는 방식으로 동작하는데 풀 방식을 채택한 이유도 리플리케이션 동작에서 리더의 부하를 줄여주기 위해서이다.


## 4.1.5 리더에포크와 복구

> 리더에포크

카프카의 파티션들이 복구 동작을 할 때 메시지의 일관성을 유지하기 위한 용도로 이용된다.
컨트롤러에 의해 관리되는 32비트의 숫자로 표현되며, 리플리케이션 프로토콜에 의해 전파되고, 새로운 리더가 변경된 후 변경된 리더에 대한 정보는 팔로워에게 전달된다. 

리더에포크는 복구 동작 시 하이워터마크를 대체하는 수단으로도 활용된다. 

> 그럼 브로커가 복구 동작을 하는데 왜 리더에포크가 필요할까?

![[Pasted image 20240617152945.png]]

`peter-test01` 토픽을 나타낸 그림 4-8 에서 파티션 수는 1, 리플리케이션 팩터 수는 2, `min.insync.replicas` 는 1이다.

1. 리더는 프로듀서로부터 message1 메시지를 받았고, 0번 오프셋에 저장, 팔로워는 리더에게 0번 오프셋에 대한 가져오기 요청을 한다.
2. 가져오기 요청을 통해 팔로워는 message1 메시지를 리더로부터 리플리케이션하낟.
3. 리더는 하이워터마크를 1로 올린다.
4. 리더는 프로듀서로부터 다음 메시지인 message2 를 받은 뒤 1번 오프셋에 저장한다.
5. 팔로워는 다음 메시지인 message2 에 대해 리더에게 가져오기 요청을 보내고, 응답으로 리더의 하위워터마크 변화를 감지하고 자신의 하이워터마크도 1로 올린다.
6. 팔로워는 1번 오프셋의 message2 메시지를 리더로부터 리플리케이션한다.
7. 팔로워는 2번 오프셋에 대한 요청을 리더에게 보내고, 요청을 받은 리더는 하이워터마크를 2로 올린다.
8. 팔로워는 2번 오프셋인 message2 메시지까지 리플리케이션을 완료했지만 아직 리더로부터 하이워터마크를 2로 올리는 내용은 전달받지 못한 상태이다.
9. 예상하지 못한 장애로 팔로워가 다운된다.

![[Pasted image 20240617153250.png]]
	장애가 발생한 팔로워가 종료된 후 장애처리가 완료된 상태를 나타낸다. 장애에서 복구된 팔로워는 카프카 프로세스가 시작되면서 내부적으로 메시지 복구 동작을 하게 된다.

1. 팔로워는 자신이 갖고 있는 메시지들 중에서 자신의 워터마크보다 높은 메시지들은 신뢰할 수 없는 메시지로 판단하고 삭제한다.
2. 팔로워는 리더에게 1번 오프셋의 새로운 메시지에 대한 가져오기 요청을 보낸다.
3. 리더였던 브로커가 예상치 못한 장애로 다운되면서, 해당 파티션에 유일하게 남아있던 팔로워가 새로운 리더로 승격된다.

![[Pasted image 20240617153435.png]]

리더와 팔로워 간의 리플리케이션이 있음에도 불구하고, 리더가 변경되는 과정을 통해 최종적으로 1번 오프셋의 message2 메시지가 손실된 것이다.

> 복구 동작 시 메시지 일관성을 유지하기 위해 리더에포크의 활용

![[Pasted image 20240617154613.png]]
	리더와 팔로워의 리플리케이션 동작 이후, 팔로워가 장애로 종료된 후 막 복구된 상태 이후의 과정이다.

리더에포크를 사용하는 경우에는 하이워터마크보다 앞에 있는 메시지를 무조건 삭제하는 것이 아니라 리더에게 리더에포크 요청을 보낸다.

1. 팔로워는 복구 동작을 하면서 리더에게 리더에포크 요청을 보낸다.
2. 요청을 받은 리더는 리더에포크의 응답으로 '1번 오프셋의 message2까지' 라고 팔로워에게 보낸다.
3. 팔로워는 자신의 하이워터마크보다 높은 1번 오프셋의 message2 를 삭제하지 않고, 리더의 응답을 확인한 후 message2 까지 자신의 하이워터마크를 상향조정한다.

![[Pasted image 20240617154824.png]]
	리더가 예상치 못한 장애로 다운되면서 팔로워가 새로운 리더로 승격된 후의 상태이다.

리더에포크를 적용하지 않은 경우에는 팔로워가 message2 메시지를 갖고 있음에도 복구 과정에서 하이워터마크보다 높은 메시지를 삭제했다. 하지만 리더에포크를 활용하는 경우에는 삭제 동작을 하기에 앞서 리더에포크 요청과 응답 과정을 통해 팔로워이 하이워터마크를 올릴 수 있었고 메시지 손실은 발생하지 않았다.

# 4.2 컨트롤러


# 4.3 로그 (로그 세그먼트)
## 4.3.1 로그 세그먼트 삭제

## 4.3.2 로그 세그먼트 컴팩션


